{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: This notebook will only work with fastai-0.7.x. Do not try to run any fastai-1.x code from this path in the repository because it will load fastai-0.7.x**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course is being taught by Jeremy Howard, and was developed by Jeremy along with Rachel Thomas. Rachel has been dealing with a life-threatening illness so will not be teaching as originally planned this year.\n",
    "\n",
    "Jeremy has worked in a number of different areas - feel free to ask about anything that he might be able to help you with at any time, even if not directly related to the current topic:\n",
    "\n",
    "- Management consultant (McKinsey; AT Kearney)\n",
    "- Self-funded startup entrepreneur (Fastmail: first consumer synchronized email; Optimal Decisions: first optimized insurance pricing)\n",
    "- VC-funded startup entrepreneur: (Kaggle; Enlitic: first deep-learning medical company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using a *top-down* teaching method, which is different from how most math courses operate.  Typically, in a *bottom-up* approach, you first learn all the separate components you will be using, and then you gradually build them up into more complex structures.  The problems with this are that students often lose motivation, don't have a sense of the \"big picture\", and don't know what they'll need.\n",
    "\n",
    "If you took the fast.ai deep learning course, that is what we used.  You can hear more about my teaching philosophy [in this blog post](http://www.fast.ai/2016/10/08/teaching-philosophy/) or [in this talk](https://vimeo.com/214233053).\n",
    "\n",
    "Harvard Professor David Perkins has a book, [Making Learning Whole](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719) in which he uses baseball as an analogy.  We don't require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game.  Rather, they start playing with a just general sense of it, and then gradually learn more rules/details as time goes on.\n",
    "\n",
    "All that to say, don't worry if you don't understand everything at first!  You're not supposed to.  We will start using some \"black boxes\" such as random forests that haven't yet been explained in detail, and then we'll dig into the lower level details later.\n",
    "\n",
    "To start, focus on what things DO, not what they ARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People learn by:\n",
    "1. **doing** (coding and building)\n",
    "2. **explaining** what they've learned (by writing or helping others)\n",
    "\n",
    "Therefore, we suggest that you practice these skills on Kaggle by:\n",
    "1. Entering competitions (*doing*)\n",
    "2. Creating Kaggle kernels (*explaining*)\n",
    "\n",
    "It's OK if you don't get good competition ranks or any kernel votes at first - that's totally normal! Just try to keep improving every day, and you'll see the results over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better at technical writing, study the top ranked Kaggle kernels from past competitions, and read posts from well-regarded technical bloggers. Some good role models include:\n",
    "\n",
    "- [Peter Norvig](http://nbviewer.jupyter.org/url/norvig.com/ipython/ProbabilityParadox.ipynb) (more [here](http://norvig.com/ipython/))\n",
    "- [Stephen Merity](https://smerity.com/articles/2017/deepcoder_and_ai_hype.html)\n",
    "- [Julia Evans](https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture) (more [here](https://jvns.ca/blog/2014/08/12/what-happens-if-you-write-a-tcp-stack-in-python/))\n",
    "- [Julia Ferraioli](http://blog.juliaferraioli.com/2016/02/exploring-world-using-vision-twilio.html)\n",
    "- [Edwin Chen](http://blog.echen.me/2014/10/07/moving-beyond-ctr-better-recommendations-through-human-evaluation/)\n",
    "- [Slav Ivanov](https://blog.slavv.com/picking-an-optimizer-for-style-transfer-86e7b8cba84b) (fast.ai student)\n",
    "- [Brad Kenstler](https://hackernoon.com/non-artistic-style-transfer-or-how-to-draw-kanye-using-captain-picards-face-c4a50256b814) (fast.ai and USF MSAN student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more familiarity you have with numeric programming in Python, the better. If you're looking to improve in this area, we strongly suggest Wes McKinney's [Python for Data Analysis, 2nd ed](https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1491957662/ref=asap_bc?ie=UTF8).\n",
    "\n",
    "For machine learning with Python, we recommend:\n",
    "\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Andreas-Mueller/dp/1449369413): From one of the scikit-learn authors, which is the main library we'll be using\n",
    "- [Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow, 2nd Edition](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939/ref=dp_ob_title_bk): New version of a very successful book. A lot of the new material however covers deep learning in Tensorflow, which isn't relevant to this course\n",
    "- [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&psc=1&refRID=MBV2QMFH3EZ6B3YBY40K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllabus in brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on time and class interests, we'll cover something like (not necessarily in this order):\n",
    "\n",
    "- Train vs test\n",
    "  - Effective validation set construction\n",
    "- Trees and ensembles\n",
    "  - Creating random forests\n",
    "  - Interpreting random forests\n",
    "- What is ML?  Why do we use it?\n",
    "  - What makes a good ML project?\n",
    "  - Structured vs unstructured data\n",
    "  - Examples of failures/mistakes\n",
    "- Feature engineering\n",
    "  - Domain specific - dates, URLs, text\n",
    "  - Embeddings / latent factors\n",
    "- Regularized models trained with SGD\n",
    "  - GLMs, Elasticnet, etc (NB: see what James covered)\n",
    "- Basic neural nets\n",
    "  - PyTorch\n",
    "  - Broadcasting, Matrix Multiplication\n",
    "  - Training loop, backpropagation\n",
    "- KNN\n",
    "- CV / bootstrap (Diabetes data set?)\n",
    "- Ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip:\n",
    "\n",
    "- Dimensionality reduction\n",
    "- Interactions\n",
    "- Monitoring training\n",
    "- Collaborative filtering\n",
    "- Momentum and LR annealing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hafiz/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from fastai.structured import *\n",
    "\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../../../AI-Med-DS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "?display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "??display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHANESI_subset_X.csv  NHANESI_subset_y.csv  pbc.csv  X_data.csv  y_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to *Blue Book for Bulldozers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...our teaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At fast.ai we have a distinctive [teaching philosophy](http://www.fast.ai/2016/10/08/teaching-philosophy/) of [\"the whole game\"](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719/ref=sr_1_1?ie=UTF8&qid=1505094653).  This is different from how most traditional math & technical courses are taught, where you have to learn all the individual elements before you can combine them (Harvard professor David Perkins call this *elementitis*), but it is similar to how topics like *driving* and *baseball* are taught.  That is, you can start driving without [knowing how an internal combustion engine works](https://medium.com/towards-data-science/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153), and children begin playing baseball before they learn all the formal rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...our approach to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning courses will throw at you dozens of different algorithms, with a brief technical description of the math behind them, and maybe a toy example. You're left confused by the enormous range of techniques shown and have little practical understanding of how to apply them.\n",
    "\n",
    "The good news is that modern machine learning can be distilled down to a couple of key techniques that are of very wide applicability. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:\n",
    "\n",
    "- *Ensembles of decision trees* (i.e. Random Forests and Gradient Boosting Machines), mainly for structured data (such as you might find in a database table at most companies)\n",
    "- *Multi-layered neural networks learnt with SGD* (i.e. shallow and/or deep learning), mainly for unstructured data (such as audio, vision, and natural language)\n",
    "\n",
    "In this course we'll be doing a deep dive into random forests, and simple models learnt with SGD. You'll be learning about gradient boosting and deep learning in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at the Blue Book for Bulldozers Kaggle Competition: \"The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.\"\n",
    "\n",
    "This is a very common type of dataset and prediciton problem, and similar to what you may see in your project or workplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...Kaggle Competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills.  There is nothing like being able to get hands-on practice and receiving real-time feedback to help you improve your skills.\n",
    "\n",
    "Kaggle provides:\n",
    "\n",
    "1. Interesting data sets\n",
    "2. Feedback on how you're doing\n",
    "3. A leader board to see what's good, what's possible, and what's state-of-art.\n",
    "4. Blog posts by winning contestants share useful tips and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle provides info about some of the fields of our dataset; on the [Kaggle Data info](https://www.kaggle.com/c/bluebook-for-bulldozers/data) page they say the following:\n",
    "\n",
    "For this competition, you are predicting the sale price of bulldozers sold at auctions. The data for this competition is split into three parts:\n",
    "\n",
    "- **Train.csv** is the training set, which contains data through the end of 2011.\n",
    "- **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012. You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n",
    "- **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n",
    "\n",
    "The key fields are in train.csv are:\n",
    "\n",
    "- SalesID: the unique identifier of the sale\n",
    "- MachineID: the unique identifier of a machine.  A machine can be sold multiple times\n",
    "- saleprice: what the machine sold for at auction (only provided in train.csv)\n",
    "- saledate: the date of the sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*\n",
    "\n",
    "What stands out to you from the above description?  What needs to be true of our training and validation sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",Age,Diastolic BP,Poverty index,Race,Red blood cells,Sedimentation rate,Serum Albumin,Serum Cholesterol,Serum Iron,Serum Magnesium,Serum Protein,Sex,Systolic BP,TIBC,TS,White blood cells,BMI,Pulse pressure\r\n",
      "0,35.0,92.0,126.0,2.0,77.7,12.0,5.0,165.0,135.0,1.37,7.6000000000000005,2.0,142.0,323.0,41.800000000000004,5.800000000000001,31.1094335693461,50.0\r\n",
      "1,71.0,78.0,210.0,2.0,77.7,37.0,4.0,298.0,89.0,1.3800000000000001,6.4,2.0,156.0,331.0,26.900000000000002,5.300000000000001,32.362571547903926,78.0\r\n",
      "2,74.0,86.0,999.0,2.0,77.7,31.0,3.8000000000000003,222.0,115.0,1.37,7.4,2.0,170.0,299.0,38.5,8.1,25.388496935587245,84.0\r\n",
      "3,64.0,92.0,385.0,1.0,77.7,30.0,4.3,265.0,94.0,1.97,7.300000000000001,2.0,172.0,349.0,26.900000000000002,6.7,26.4466104106932,80.0\r\n",
      "4,32.0,70.0,183.0,2.0,77.7,18.0,5.0,203.0,192.0,1.35,7.300000000000001,1.0,128.0,386.0,49.7,8.1,20.354684270159435,58.0\r\n",
      "5,40.0,78.0,297.0,2.0,77.7,24.0,4.0,173.0,121.0,1.71,6.7,2.0,118.0,370.0,32.7,10.700000000000001,27.21720136834319,40.0\r\n",
      "6,53.0,76.0,461.0,1.0,77.7,2.0,4.3,276.0,135.0,1.74,7.6000000000000005,1.0,124.0,334.0,40.400000000000006,6.0,23.09182320004831,48.0\r\n",
      "7,36.0,66.0,423.0,1.0,77.7,11.0,4.2,160.0,107.0,1.69,6.300000000000001,2.0,108.0,289.0,37.0,9.5,20.313169017229285,42.0\r\n",
      "8,49.0,86.0,385.0,1.0,77.7,34.0,4.3,326.0,85.0,1.62,7.300000000000001,1.0,134.0,385.0,22.1,6.4,30.004221578147703,48.0\r\n"
     ]
    }
   ],
   "source": [
    "!head '{PATH}NHANESI_subset_X.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Hassan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HelloHassan'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Hello{name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "age =23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello HASSAN , you are of 23'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Hello {name.upper()} , you are of {age}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas f'{PATH}Train.csv'lets use csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../AI-Med-DS/NHANESI_subset_X.csv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{PATH}NHANESI_subset_X.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9933 ../../../AI-Med-DS/NHANESI_subset_X.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l '{PATH}NHANESI_subset_X.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pandas merging two csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(f'{PATH}NHANESI_subset_X.csv')\n",
    "df2 = pd.read_csv(f'{PATH}NHANESI_subset_y.csv')\n",
    "\n",
    "df = df1.merge(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9927</th>\n",
       "      <th>9928</th>\n",
       "      <th>9929</th>\n",
       "      <th>9930</th>\n",
       "      <th>9931</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>9927.000000</td>\n",
       "      <td>9928.000000</td>\n",
       "      <td>9929.000000</td>\n",
       "      <td>9930.000000</td>\n",
       "      <td>9931.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diastolic BP</th>\n",
       "      <td>76.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poverty index</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>316.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Race</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Red blood cells</th>\n",
       "      <td>46.600000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>47.300000</td>\n",
       "      <td>54.700000</td>\n",
       "      <td>48.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sedimentation rate</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Albumin</th>\n",
       "      <td>4.700000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Cholesterol</th>\n",
       "      <td>137.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Iron</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>124.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Magnesium</th>\n",
       "      <td>1.390000</td>\n",
       "      <td>2.020000</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>1.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Protein</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Systolic BP</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>132.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIBC</th>\n",
       "      <td>316.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>29.300000</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>31.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White blood cells</th>\n",
       "      <td>10.700000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>21.468805</td>\n",
       "      <td>23.090226</td>\n",
       "      <td>25.797680</td>\n",
       "      <td>23.406200</td>\n",
       "      <td>28.763104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulse pressure</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>9.939726</td>\n",
       "      <td>-18.913927</td>\n",
       "      <td>-18.877169</td>\n",
       "      <td>-19.683105</td>\n",
       "      <td>17.938584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           9927         9928         9929         9930  \\\n",
       "Unnamed: 0          9927.000000  9928.000000  9929.000000  9930.000000   \n",
       "Age                   26.000000    69.000000    34.000000    57.000000   \n",
       "Diastolic BP          76.000000    82.000000    80.000000    84.000000   \n",
       "Poverty index        491.000000   316.000000   187.000000   999.000000   \n",
       "Race                   1.000000     1.000000     1.000000     1.000000   \n",
       "Red blood cells       46.600000    48.000000    47.300000    54.700000   \n",
       "Sedimentation rate     9.000000    18.000000    10.000000     5.000000   \n",
       "Serum Albumin          4.700000     4.700000     4.500000     4.500000   \n",
       "Serum Cholesterol    137.000000   301.000000   161.000000   210.000000   \n",
       "Serum Iron            90.000000    98.000000    95.000000    93.000000   \n",
       "Serum Magnesium        1.390000     2.020000     1.540000     1.820000   \n",
       "Serum Protein          7.400000     6.700000     6.900000     7.000000   \n",
       "Sex                    1.000000     2.000000     2.000000     1.000000   \n",
       "Systolic BP          104.000000   130.000000   110.000000   130.000000   \n",
       "TIBC                 316.000000   334.000000   381.000000   337.000000   \n",
       "TS                    28.500000    29.300000    24.900000    27.600000   \n",
       "White blood cells     10.700000     7.100000     5.400000     6.200000   \n",
       "BMI                   21.468805    23.090226    25.797680    23.406200   \n",
       "Pulse pressure        28.000000    48.000000    30.000000    46.000000   \n",
       "y                      9.939726   -18.913927   -18.877169   -19.683105   \n",
       "\n",
       "                           9931  \n",
       "Unnamed: 0          9931.000000  \n",
       "Age                   70.000000  \n",
       "Diastolic BP          70.000000  \n",
       "Poverty index        204.000000  \n",
       "Race                   1.000000  \n",
       "Red blood cells       48.800000  \n",
       "Sedimentation rate     9.000000  \n",
       "Serum Albumin          4.400000  \n",
       "Serum Cholesterol    205.000000  \n",
       "Serum Iron           124.000000  \n",
       "Serum Magnesium        1.910000  \n",
       "Serum Protein          6.800000  \n",
       "Sex                    1.000000  \n",
       "Systolic BP          132.000000  \n",
       "TIBC                 398.000000  \n",
       "TS                    31.200000  \n",
       "White blood cells      7.500000  \n",
       "BMI                   28.763104  \n",
       "Pulse pressure        62.000000  \n",
       "y                     17.938584  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_all(df.tail().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Unnamed: 0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Unnamed: 0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f36b13d8c751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__delitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3757\u001b[0m             \u001b[0;31m# there was no match, this call should raise the appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3758\u001b[0m             \u001b[0;31m# exception:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3761\u001b[0m         \u001b[0;31m# delete from the caches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mselected\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \"\"\"\n\u001b[0;32m-> 1002\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0mis_deleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Unnamed: 0'"
     ]
    }
   ],
   "source": [
    "del df['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw = pd.read_csv(f'{PATH}NHANESI_subset_X.csv', low_memory=False, \n",
    "#                     parse_dates=[\"saledate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any sort of data science work, it's **important to look at your data**, to make sure you understand the format, how it's stored, what type of values it holds, etc. Even if you've read descriptions about your data, the actual data may not be what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9927</th>\n",
       "      <th>9928</th>\n",
       "      <th>9929</th>\n",
       "      <th>9930</th>\n",
       "      <th>9931</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diastolic BP</th>\n",
       "      <td>76.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poverty index</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>316.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Race</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Red blood cells</th>\n",
       "      <td>46.600000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>47.300000</td>\n",
       "      <td>54.700000</td>\n",
       "      <td>48.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sedimentation rate</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Albumin</th>\n",
       "      <td>4.700000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Cholesterol</th>\n",
       "      <td>137.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Iron</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>124.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Magnesium</th>\n",
       "      <td>1.390000</td>\n",
       "      <td>2.020000</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>1.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Protein</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Systolic BP</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>132.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIBC</th>\n",
       "      <td>316.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>29.300000</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>31.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White blood cells</th>\n",
       "      <td>10.700000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>21.468805</td>\n",
       "      <td>23.090226</td>\n",
       "      <td>25.797680</td>\n",
       "      <td>23.406200</td>\n",
       "      <td>28.763104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulse pressure</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>9.939726</td>\n",
       "      <td>-18.913927</td>\n",
       "      <td>-18.877169</td>\n",
       "      <td>-19.683105</td>\n",
       "      <td>17.938584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          9927        9928        9929        9930        9931\n",
       "Age                  26.000000   69.000000   34.000000   57.000000   70.000000\n",
       "Diastolic BP         76.000000   82.000000   80.000000   84.000000   70.000000\n",
       "Poverty index       491.000000  316.000000  187.000000  999.000000  204.000000\n",
       "Race                  1.000000    1.000000    1.000000    1.000000    1.000000\n",
       "Red blood cells      46.600000   48.000000   47.300000   54.700000   48.800000\n",
       "Sedimentation rate    9.000000   18.000000   10.000000    5.000000    9.000000\n",
       "Serum Albumin         4.700000    4.700000    4.500000    4.500000    4.400000\n",
       "Serum Cholesterol   137.000000  301.000000  161.000000  210.000000  205.000000\n",
       "Serum Iron           90.000000   98.000000   95.000000   93.000000  124.000000\n",
       "Serum Magnesium       1.390000    2.020000    1.540000    1.820000    1.910000\n",
       "Serum Protein         7.400000    6.700000    6.900000    7.000000    6.800000\n",
       "Sex                   1.000000    2.000000    2.000000    1.000000    1.000000\n",
       "Systolic BP         104.000000  130.000000  110.000000  130.000000  132.000000\n",
       "TIBC                316.000000  334.000000  381.000000  337.000000  398.000000\n",
       "TS                   28.500000   29.300000   24.900000   27.600000   31.200000\n",
       "White blood cells    10.700000    7.100000    5.400000    6.200000    7.500000\n",
       "BMI                  21.468805   23.090226   25.797680   23.406200   28.763104\n",
       "Pulse pressure       28.000000   48.000000   30.000000   46.000000   62.000000\n",
       "y                     9.939726  -18.913927  -18.877169  -19.683105   17.938584"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#transpose column as rows \n",
    "display_all(df.tail().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>49.449054</td>\n",
       "      <td>15.878425</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diastolic BP</th>\n",
       "      <td>9874.0</td>\n",
       "      <td>83.281953</td>\n",
       "      <td>13.291925</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poverty index</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>287.213049</td>\n",
       "      <td>223.310410</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Race</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>1.196033</td>\n",
       "      <td>0.423045</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Red blood cells</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>54.880528</td>\n",
       "      <td>14.600767</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>48.900000</td>\n",
       "      <td>57.600000</td>\n",
       "      <td>88.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sedimentation rate</th>\n",
       "      <td>9106.0</td>\n",
       "      <td>16.268285</td>\n",
       "      <td>11.511728</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Albumin</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>4.365103</td>\n",
       "      <td>0.331212</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>5.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Cholesterol</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>221.234837</td>\n",
       "      <td>49.494473</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>793.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Iron</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>101.117298</td>\n",
       "      <td>37.211666</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Magnesium</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>1.681307</td>\n",
       "      <td>0.145905</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>1.590000</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>2.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serum Protein</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>7.104068</td>\n",
       "      <td>0.508969</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>11.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>1.607229</td>\n",
       "      <td>0.488391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Systolic BP</th>\n",
       "      <td>9875.0</td>\n",
       "      <td>134.854481</td>\n",
       "      <td>24.932032</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>270.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIBC</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>362.604511</td>\n",
       "      <td>58.993649</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>717.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>28.555910</td>\n",
       "      <td>11.232671</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>34.600000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White blood cells</th>\n",
       "      <td>8891.0</td>\n",
       "      <td>7.451760</td>\n",
       "      <td>2.292071</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>25.689771</td>\n",
       "      <td>5.184155</td>\n",
       "      <td>12.585333</td>\n",
       "      <td>22.108338</td>\n",
       "      <td>24.928952</td>\n",
       "      <td>28.369043</td>\n",
       "      <td>72.218113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulse pressure</th>\n",
       "      <td>9873.0</td>\n",
       "      <td>51.575509</td>\n",
       "      <td>18.299075</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>170.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>9932.0</td>\n",
       "      <td>-8.138465</td>\n",
       "      <td>14.848999</td>\n",
       "      <td>-22.058676</td>\n",
       "      <td>-20.103653</td>\n",
       "      <td>-18.710502</td>\n",
       "      <td>7.006450</td>\n",
       "      <td>21.471461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count        mean         std         min         25%  \\\n",
       "Age                 9932.0   49.449054   15.878425   25.000000   35.000000   \n",
       "Diastolic BP        9874.0   83.281953   13.291925   25.000000   74.000000   \n",
       "Poverty index       9932.0  287.213049  223.310410    2.000000  130.000000   \n",
       "Race                9932.0    1.196033    0.423045    1.000000    1.000000   \n",
       "Red blood cells     9932.0   54.880528   14.600767   21.400000   45.000000   \n",
       "Sedimentation rate  9106.0   16.268285   11.511728    1.000000    7.000000   \n",
       "Serum Albumin       9932.0    4.365103    0.331212    2.700000    4.200000   \n",
       "Serum Cholesterol   9932.0  221.234837   49.494473   53.000000  187.000000   \n",
       "Serum Iron          9932.0  101.117298   37.211666   17.000000   75.000000   \n",
       "Serum Magnesium     9932.0    1.681307    0.145905    0.820000    1.590000   \n",
       "Serum Protein       9932.0    7.104068    0.508969    4.400000    6.800000   \n",
       "Sex                 9932.0    1.607229    0.488391    1.000000    1.000000   \n",
       "Systolic BP         9875.0  134.854481   24.932032   80.000000  118.000000   \n",
       "TIBC                9932.0  362.604511   58.993649  112.000000  322.000000   \n",
       "TS                  9932.0   28.555910   11.232671    3.200000   21.000000   \n",
       "White blood cells   8891.0    7.451760    2.292071    2.100000    6.000000   \n",
       "BMI                 9932.0   25.689771    5.184155   12.585333   22.108338   \n",
       "Pulse pressure      9873.0   51.575509   18.299075   10.000000   40.000000   \n",
       "y                   9932.0   -8.138465   14.848999  -22.058676  -20.103653   \n",
       "\n",
       "                           50%         75%         max  \n",
       "Age                  48.000000   66.000000   74.000000  \n",
       "Diastolic BP         82.000000   90.000000  180.000000  \n",
       "Poverty index       233.000000  372.000000  999.000000  \n",
       "Race                  1.000000    1.000000    3.000000  \n",
       "Red blood cells      48.900000   57.600000   88.800000  \n",
       "Sedimentation rate   14.000000   22.000000   72.000000  \n",
       "Serum Albumin         4.400000    4.600000    5.700000  \n",
       "Serum Cholesterol   217.000000  250.000000  793.000000  \n",
       "Serum Iron           96.000000  122.000000  396.000000  \n",
       "Serum Magnesium       1.680000    1.770000    2.890000  \n",
       "Serum Protein         7.100000    7.400000   11.500000  \n",
       "Sex                   2.000000    2.000000    2.000000  \n",
       "Systolic BP         130.000000  150.000000  270.000000  \n",
       "TIBC                356.000000  396.000000  717.000000  \n",
       "TS                   27.200000   34.600000  100.000000  \n",
       "White blood cells     7.200000    8.600000   56.000000  \n",
       "BMI                  24.928952   28.369043   72.218113  \n",
       "Pulse pressure       48.000000   60.000000  170.000000  \n",
       "y                   -18.710502    7.006450   21.471461  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw =df\n",
    "display_all(df_raw.describe(include='all').T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.ensemble._forest.RandomForestRegressor"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier for categorical and regressor for continuius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-22b183dc42d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The following code is supposed to fail due to string values in the input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# m.fit(idependent, dependent )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1) #because we are using continious variable to predict\n",
    "#other wise classifier used for category \n",
    "\n",
    "# The following code is supposed to fail due to string values in the input data\n",
    "m.fit(df_raw.drop('y', axis=1), df_raw.y)\n",
    "\n",
    "# m.fit(idependent, dependent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a mix of **continuous** and **categorical** variables.\n",
    "\n",
    "The following method extracts particular date fields from a complete datetime for the purpose of constructing categoricals.  You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables are currently stored as strings, which is inefficient, and doesn't provide the numeric coding required for a random forest. Therefore we call `train_cats` to convert strings to pandas categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Diastolic BP', 'Poverty index', 'Race', 'Red blood cells',\n",
       "       'Sedimentation rate', 'Serum Albumin', 'Serum Cholesterol',\n",
       "       'Serum Iron', 'Serum Magnesium', 'Serum Protein', 'Sex', 'Systolic BP',\n",
       "       'TIBC', 'TS', 'White blood cells', 'BMI', 'Pulse pressure', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Diastolic BP</th>\n",
       "      <th>Poverty index</th>\n",
       "      <th>Race</th>\n",
       "      <th>Red blood cells</th>\n",
       "      <th>Sedimentation rate</th>\n",
       "      <th>Serum Albumin</th>\n",
       "      <th>Serum Cholesterol</th>\n",
       "      <th>Serum Iron</th>\n",
       "      <th>Serum Magnesium</th>\n",
       "      <th>Serum Protein</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Systolic BP</th>\n",
       "      <th>TIBC</th>\n",
       "      <th>TS</th>\n",
       "      <th>White blood cells</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Pulse pressure</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1.37</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>31.109434</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15.274658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>5.3</td>\n",
       "      <td>32.362572</td>\n",
       "      <td>78.0</td>\n",
       "      <td>11.586073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>222.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.37</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>25.388497</td>\n",
       "      <td>84.0</td>\n",
       "      <td>8.149087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>265.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.97</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>26.446610</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-21.094292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.35</td>\n",
       "      <td>7.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>49.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>20.354684</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Diastolic BP  Poverty index  Race  Red blood cells  \\\n",
       "0  35.0          92.0          126.0   2.0             77.7   \n",
       "1  71.0          78.0          210.0   2.0             77.7   \n",
       "2  74.0          86.0          999.0   2.0             77.7   \n",
       "3  64.0          92.0          385.0   1.0             77.7   \n",
       "4  32.0          70.0          183.0   2.0             77.7   \n",
       "\n",
       "   Sedimentation rate  Serum Albumin  Serum Cholesterol  Serum Iron  \\\n",
       "0                12.0            5.0              165.0       135.0   \n",
       "1                37.0            4.0              298.0        89.0   \n",
       "2                31.0            3.8              222.0       115.0   \n",
       "3                30.0            4.3              265.0        94.0   \n",
       "4                18.0            5.0              203.0       192.0   \n",
       "\n",
       "   Serum Magnesium  Serum Protein  Sex  Systolic BP   TIBC    TS  \\\n",
       "0             1.37            7.6  2.0        142.0  323.0  41.8   \n",
       "1             1.38            6.4  2.0        156.0  331.0  26.9   \n",
       "2             1.37            7.4  2.0        170.0  299.0  38.5   \n",
       "3             1.97            7.3  2.0        172.0  349.0  26.9   \n",
       "4             1.35            7.3  1.0        128.0  386.0  49.7   \n",
       "\n",
       "   White blood cells        BMI  Pulse pressure          y  \n",
       "0                5.8  31.109434            50.0  15.274658  \n",
       "1                5.3  32.362572            78.0  11.586073  \n",
       "2                8.1  25.388497            84.0   8.149087  \n",
       "3                6.7  26.446610            80.0 -21.094292  \n",
       "4                8.1  20.354684            58.0  -0.000000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See USAGE BAND Need to Categories to Integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember Test Set Maapping should be same as Training Set mapping\n",
    " Train_cats Creates categorical variable for df columns having type string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two make the same training and testing mapping same ??apply_cats Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_cats(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We can specify the order to use for categorical variables if we wish:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, pandas will continue displaying the text categories, while treating them as numerical data internally. Optionally, we can replace the text categories with numbers, which will make this variable non-categorical, like so:."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still not quite done - for instance we have lots of missing values, which we can't pass directly to a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's save this file for now, since it's already in format can we be stored and accessed efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)\n",
    "#Saving into ram\n",
    "df_raw.to_feather('tmp/medical-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we can simply read it from this fast format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_feather('tmp/medical-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "??proc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if missing value is numeric replace it with new boolean column_na\n",
    "#(1 means missed 0 means not missed  ) plus original column with median value\n",
    "#categories not need to handle missing assigned  codes = codes +1 \n",
    "#so missing value by default was zero now it becomes 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "??fix_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proc_df issue \n",
    "An interesting little issue that was brought up during the week is in proc_df function. proc_df function does the following:\n",
    "Finds numeric columns which have missing values and create an additional boolean column as well as replacing the missing with medians.\n",
    "Turn the categorical objects into integer codes.\n",
    "\n",
    "### Problem #1: \n",
    "Your test set may have missing values in some columns that were not in your training set or vice versa. If that happens, you are going to get an error when you try to do the random forest since the “missing” boolean column appeared in your training set but not in the test set.\n",
    "\n",
    "### Problem #2: \n",
    "Median of the numeric value in the test set may be different from the training set. So it may process it into something which has different semantics.\n",
    "### Solution: \n",
    "There is now an additional return variable nas from proc_df which is a dictionary whose keys are the names of the columns that had missing values, and the values of the dictionary are the medians. Optionally, you can pass nas to proc_df as an argument to make sure that it adds those specific columns and uses those specific medians:\n",
    "\n",
    "#### df, y, nas = proc_df(df_raw, 'SalePrice', nas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df, y, nas = proc_df(df_raw, 'y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have something we can pass to a random forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Diastolic BP</th>\n",
       "      <th>Poverty index</th>\n",
       "      <th>Race</th>\n",
       "      <th>Red blood cells</th>\n",
       "      <th>Sedimentation rate</th>\n",
       "      <th>Serum Albumin</th>\n",
       "      <th>Serum Cholesterol</th>\n",
       "      <th>Serum Iron</th>\n",
       "      <th>Serum Magnesium</th>\n",
       "      <th>...</th>\n",
       "      <th>TIBC</th>\n",
       "      <th>TS</th>\n",
       "      <th>White blood cells</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Pulse pressure</th>\n",
       "      <th>Diastolic BP_na</th>\n",
       "      <th>Sedimentation rate_na</th>\n",
       "      <th>Systolic BP_na</th>\n",
       "      <th>White blood cells_na</th>\n",
       "      <th>Pulse pressure_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1.37</td>\n",
       "      <td>...</td>\n",
       "      <td>323.0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>31.109434</td>\n",
       "      <td>50.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>...</td>\n",
       "      <td>331.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>5.3</td>\n",
       "      <td>32.362572</td>\n",
       "      <td>78.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>222.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.37</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>25.388497</td>\n",
       "      <td>84.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>265.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>349.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>26.446610</td>\n",
       "      <td>80.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>386.0</td>\n",
       "      <td>49.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>20.354684</td>\n",
       "      <td>58.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Diastolic BP  Poverty index  Race  Red blood cells  \\\n",
       "0  35.0          92.0          126.0   2.0             77.7   \n",
       "1  71.0          78.0          210.0   2.0             77.7   \n",
       "2  74.0          86.0          999.0   2.0             77.7   \n",
       "3  64.0          92.0          385.0   1.0             77.7   \n",
       "4  32.0          70.0          183.0   2.0             77.7   \n",
       "\n",
       "   Sedimentation rate  Serum Albumin  Serum Cholesterol  Serum Iron  \\\n",
       "0                12.0            5.0              165.0       135.0   \n",
       "1                37.0            4.0              298.0        89.0   \n",
       "2                31.0            3.8              222.0       115.0   \n",
       "3                30.0            4.3              265.0        94.0   \n",
       "4                18.0            5.0              203.0       192.0   \n",
       "\n",
       "   Serum Magnesium  ...   TIBC    TS  White blood cells        BMI  \\\n",
       "0             1.37  ...  323.0  41.8                5.8  31.109434   \n",
       "1             1.38  ...  331.0  26.9                5.3  32.362572   \n",
       "2             1.37  ...  299.0  38.5                8.1  25.388497   \n",
       "3             1.97  ...  349.0  26.9                6.7  26.446610   \n",
       "4             1.35  ...  386.0  49.7                8.1  20.354684   \n",
       "\n",
       "   Pulse pressure  Diastolic BP_na  Sedimentation rate_na  Systolic BP_na  \\\n",
       "0            50.0            False                  False           False   \n",
       "1            78.0            False                  False           False   \n",
       "2            84.0            False                  False           False   \n",
       "3            80.0            False                  False           False   \n",
       "4            58.0            False                  False           False   \n",
       "\n",
       "   White blood cells_na  Pulse pressure_na  \n",
       "0                 False              False  \n",
       "1                 False              False  \n",
       "2                 False              False  \n",
       "3                 False              False  \n",
       "4                 False              False  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9932"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9932"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9073240691976954"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data to CPU's (seperate the job)\n",
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "m.fit(df, y)\n",
    "m.score(df,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). https://en.wikipedia.org/wiki/Coefficient_of_determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, an r^2 of 0.98 - that's great, right? Well, perhaps not...\n",
    "\n",
    "Possibly **the most important idea** in machine learning is that of having separate training & validation data sets. As motivation, suppose you don't divide up your data, but instead use all of it.  And suppose you have lots of parameters:\n",
    "\n",
    "<img src=\"images/overfitting2.png\" alt=\"\" style=\"width: 70%\"/>\n",
    "<center>\n",
    "[Underfitting and Overfitting](https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)\n",
    "</center>\n",
    "\n",
    "The error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it's not the best choice.  Why is that?  If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n",
    "\n",
    "This illustrates how using all our data can lead to **overfitting**. A validation set helps diagnose this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000, 23), (7000,), (2932, 23))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "n_valid = 9932-7000  # same as Kaggle's test set size\n",
    "n_trn = len(df)-n_valid\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "X_train, X_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our model again, this time with separate training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "# rms (train , validation), r2 score(train,valid) \n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 s, sys: 77.6 ms, total: 13.8 s\n",
      "Wall time: 4.1 s\n",
      "[4.652663172559921, 11.072309644220454, 0.9078201543841732, 0.33221492377616013]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An r^2 in the high-80's isn't bad at all (and the RMSLE puts us around rank 100 of 470 on the Kaggle leaderboard), but we can see from the validation set score that we're over-fitting badly. To understand this issue, let's simplify things down to a single small tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "randomly sample 300000 make sure it does not overlap with\n",
    "validation set\n",
    "_ means throw something away\n",
    "\n",
    "why boot strap\n",
    "\n",
    "procdf return dict with for each missing column it will tell \n",
    "what the median is pass na means it will update \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df_trn, y_trn, nas = proc_df(df_raw, 'y', subset=9932, na_dict=nas)\n",
    "X_train, _ = split_vals(df_trn, 7000)\n",
    "y_train, _ = split_vals(y_trn, 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 s, sys: 59.5 ms, total: 13.8 s\n",
      "Wall time: 4.1 s\n",
      "[4.650034325958707, 11.092554041410207, 0.9079242918233917, 0.32977076070469324]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.38895698895301, 11.196537262879957, 0.3464147529644119, 0.3171462110388108]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nI want to build random forest from scratch what I would do find \\nthe variable that we could split into such that two groups are\\ndifferent to each other as possible\\n\\nTask 1\\nwhich variable\\ntask 2\\nwhich cut point\\n\\nSolution\\nAll possible split\\n\\n\\nit means for each variable \\n    for each possible value of variable\\n        see if it is better\\n        \\n        what is better?\\n        problem spliting such a way \\n        one group have 1 row \\n        other have rest\\n        \\n        evaluate weighted average of two groups\\n        17805*0.415+ 2195 *0.112\\n        \\n        \\n        \\n        \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "forest made of trees ..scikit we call it estimators\n",
    "we just create 1 tree with max deapth =3\n",
    "dont randomize everything so bootstrap =false\n",
    "\n",
    "\n",
    "I want to build random forest from scratch what I would do find \n",
    "the variable that we could split into such that two groups are\n",
    "different to each other as possible\n",
    "\n",
    "Task 1\n",
    "which variable\n",
    "task 2\n",
    "which cut point\n",
    "\n",
    "Solution\n",
    "All possible split\n",
    "\n",
    "\n",
    "it means for each variable \n",
    "    for each possible value of variable\n",
    "        see if it is better\n",
    "        \n",
    "        what is better?\n",
    "        problem spliting such a way \n",
    "        one group have 1 row \n",
    "        other have rest\n",
    "        \n",
    "        evaluate weighted average of two groups\n",
    "        17805*0.415+ 2195 *0.112\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"720pt\" height=\"434pt\"\n",
       " viewBox=\"0.00 0.00 720.00 434.49\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(.7787 .7787) rotate(0) translate(4 554)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-554 920.6667,-554 920.6667,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"#eca36f\" stroke=\"#000000\" points=\"195.1667,-336.5 14.1667,-336.5 14.1667,-268.5 195.1667,-268.5 195.1667,-336.5\"/>\n",
       "<text text-anchor=\"start\" x=\"22.1667\" y=\"-321.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Hydraulics_Flow ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"59.1667\" y=\"-306.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.461</text>\n",
       "<text text-anchor=\"start\" x=\"42.1667\" y=\"-291.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 20000</text>\n",
       "<text text-anchor=\"start\" x=\"50.1667\" y=\"-276.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 10.112</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#eb9c63\" stroke=\"#000000\" points=\"422.6667,-372.5 264.6667,-372.5 264.6667,-304.5 422.6667,-304.5 422.6667,-372.5\"/>\n",
       "<text text-anchor=\"start\" x=\"272.6667\" y=\"-357.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">YearMade ≤ 1984.5</text>\n",
       "<text text-anchor=\"start\" x=\"298.1667\" y=\"-342.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.401</text>\n",
       "<text text-anchor=\"start\" x=\"281.1667\" y=\"-327.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 18160</text>\n",
       "<text text-anchor=\"start\" x=\"289.1667\" y=\"-312.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 10.205</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.3631,-316.1614C214.7048,-319.0748 235.117,-322.1494 254.4043,-325.0546\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"253.9664,-328.5281 264.3762,-326.5567 255.0091,-321.6062 253.9664,-328.5281\"/>\n",
       "<text text-anchor=\"middle\" x=\"244.2627\" y=\"-337.7041\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#fcf0e8\" stroke=\"#000000\" points=\"422.6667,-245.5 264.6667,-245.5 264.6667,-177.5 422.6667,-177.5 422.6667,-245.5\"/>\n",
       "<text text-anchor=\"start\" x=\"272.6667\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">YearMade ≤ 1998.5</text>\n",
       "<text text-anchor=\"start\" x=\"298.1667\" y=\"-215.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.107</text>\n",
       "<text text-anchor=\"start\" x=\"285.6667\" y=\"-200.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1840</text>\n",
       "<text text-anchor=\"start\" x=\"293.6667\" y=\"-185.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.186</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M194.0013,-268.4856C214.0318,-260.8589 235.2721,-252.7716 255.2644,-245.1594\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"256.5593,-248.4116 264.6593,-241.5823 254.0684,-241.8698 256.5593,-248.4116\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.8484\" y=\"-227.6519\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#f0b890\" stroke=\"#000000\" points=\"674.1667,-486.5 525.1667,-486.5 525.1667,-418.5 674.1667,-418.5 674.1667,-486.5\"/>\n",
       "<text text-anchor=\"start\" x=\"533.1667\" y=\"-471.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ModelID ≤ 4157.5</text>\n",
       "<text text-anchor=\"start\" x=\"554.1667\" y=\"-456.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.321</text>\n",
       "<text text-anchor=\"start\" x=\"541.6667\" y=\"-441.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 5454</text>\n",
       "<text text-anchor=\"start\" x=\"549.6667\" y=\"-426.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.858</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M420.1283,-372.5493C450.1705,-385.9275 484.7598,-401.3305 515.5094,-415.0237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"514.3609,-418.3436 524.9199,-419.2143 517.2085,-411.949 514.3609,-418.3436\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#e88f4f\" stroke=\"#000000\" points=\"700.1667,-372.5 499.1667,-372.5 499.1667,-304.5 700.1667,-304.5 700.1667,-372.5\"/>\n",
       "<text text-anchor=\"start\" x=\"507.1667\" y=\"-357.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">fiProductClassDesc ≤ 7.5</text>\n",
       "<text text-anchor=\"start\" x=\"554.1667\" y=\"-342.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.361</text>\n",
       "<text text-anchor=\"start\" x=\"537.1667\" y=\"-327.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 12706</text>\n",
       "<text text-anchor=\"start\" x=\"545.1667\" y=\"-312.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 10.355</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M422.9135,-338.5C443.7976,-338.5 466.7174,-338.5 488.9306,-338.5\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"489.0772,-342.0001 499.0771,-338.5 489.0771,-335.0001 489.0772,-342.0001\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#eda876\" stroke=\"#000000\" points=\"905.6667,-550 773.6667,-550 773.6667,-497 905.6667,-497 905.6667,-550\"/>\n",
       "<text text-anchor=\"start\" x=\"794.1667\" y=\"-534.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.348</text>\n",
       "<text text-anchor=\"start\" x=\"781.6667\" y=\"-519.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2669</text>\n",
       "<text text-anchor=\"start\" x=\"785.1667\" y=\"-504.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 10.061</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M674.2886,-474.5757C702.7223,-482.9873 735.1423,-492.5782 763.6996,-501.0264\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"763.0446,-504.4825 773.6267,-503.9632 765.0304,-497.7701 763.0446,-504.4825\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#f4c9a9\" stroke=\"#000000\" points=\"905.6667,-479 773.6667,-479 773.6667,-426 905.6667,-426 905.6667,-479\"/>\n",
       "<text text-anchor=\"start\" x=\"794.1667\" y=\"-463.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.218</text>\n",
       "<text text-anchor=\"start\" x=\"781.6667\" y=\"-448.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2785</text>\n",
       "<text text-anchor=\"start\" x=\"789.6667\" y=\"-433.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.664</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M674.2886,-452.5C702.5982,-452.5 734.8593,-452.5 763.3253,-452.5\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"763.6267,-456.0001 773.6267,-452.5 763.6266,-449.0001 763.6267,-456.0001\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"#efb185\" stroke=\"#000000\" points=\"905.6667,-408 773.6667,-408 773.6667,-355 905.6667,-355 905.6667,-408\"/>\n",
       "<text text-anchor=\"start\" x=\"794.1667\" y=\"-392.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.126</text>\n",
       "<text text-anchor=\"start\" x=\"781.6667\" y=\"-377.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3758</text>\n",
       "<text text-anchor=\"start\" x=\"789.6667\" y=\"-362.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.943</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M700.4051,-356.549C721.4748,-360.324 743.358,-364.2447 763.3644,-367.8292\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"762.8704,-371.2963 773.331,-369.6149 764.105,-364.4061 762.8704,-371.2963\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"#000000\" points=\"905.6667,-337 773.6667,-337 773.6667,-284 905.6667,-284 905.6667,-337\"/>\n",
       "<text text-anchor=\"start\" x=\"794.1667\" y=\"-321.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.359</text>\n",
       "<text text-anchor=\"start\" x=\"781.6667\" y=\"-306.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8948</text>\n",
       "<text text-anchor=\"start\" x=\"785.1667\" y=\"-291.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 10.527</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M700.4051,-326.7472C721.4748,-324.289 743.358,-321.736 763.3644,-319.4019\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"763.804,-322.8745 773.331,-318.2392 762.9927,-315.9216 763.804,-322.8745\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#fefcfa\" stroke=\"#000000\" points=\"704.6667,-245.5 494.6667,-245.5 494.6667,-177.5 704.6667,-177.5 704.6667,-245.5\"/>\n",
       "<text text-anchor=\"start\" x=\"502.6667\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">fiProductClassDesc ≤ 40.5</text>\n",
       "<text text-anchor=\"start\" x=\"554.1667\" y=\"-215.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.081</text>\n",
       "<text text-anchor=\"start\" x=\"546.1667\" y=\"-200.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 991</text>\n",
       "<text text-anchor=\"start\" x=\"549.6667\" y=\"-185.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.047</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M422.9135,-211.5C442.3475,-211.5 463.5443,-211.5 484.2942,-211.5\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"484.4349,-215.0001 494.4349,-211.5 484.4348,-208.0001 484.4349,-215.0001\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#f9e3d3\" stroke=\"#000000\" points=\"704.6667,-131.5 494.6667,-131.5 494.6667,-63.5 704.6667,-63.5 704.6667,-131.5\"/>\n",
       "<text text-anchor=\"start\" x=\"502.6667\" y=\"-116.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">fiProductClassDesc ≤ 39.5</text>\n",
       "<text text-anchor=\"start\" x=\"554.1667\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.088</text>\n",
       "<text text-anchor=\"start\" x=\"546.1667\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 849</text>\n",
       "<text text-anchor=\"start\" x=\"549.6667\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.348</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M420.1283,-177.4507C449.6364,-164.3103 483.5314,-149.2165 513.8659,-135.7082\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"515.4456,-138.8361 523.1569,-131.5707 512.5979,-132.4415 515.4456,-138.8361\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"#000000\" points=\"901.1667,-266 778.1667,-266 778.1667,-213 901.1667,-213 901.1667,-266\"/>\n",
       "<text text-anchor=\"start\" x=\"794.1667\" y=\"-250.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.067</text>\n",
       "<text text-anchor=\"start\" x=\"786.1667\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 785</text>\n",
       "<text text-anchor=\"start\" x=\"789.6667\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.006</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M704.9342,-223.7812C726.1701,-226.2587 748.0309,-228.8092 767.802,-231.1158\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"767.598,-234.6156 777.9362,-232.2981 768.4092,-227.6628 767.598,-234.6156\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"#fcefe5\" stroke=\"#000000\" points=\"901.1667,-195 778.1667,-195 778.1667,-142 901.1667,-142 901.1667,-195\"/>\n",
       "<text text-anchor=\"start\" x=\"794.1667\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.105</text>\n",
       "<text text-anchor=\"start\" x=\"786.1667\" y=\"-164.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 206</text>\n",
       "<text text-anchor=\"start\" x=\"789.6667\" y=\"-149.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.204</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M704.9342,-192.6396C726.2675,-188.8174 748.2315,-184.8821 768.074,-181.327\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"768.7102,-184.7688 777.9362,-179.56 767.4757,-177.8786 768.7102,-184.7688\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#fbeee4\" stroke=\"#000000\" points=\"901.1667,-124 778.1667,-124 778.1667,-71 901.1667,-71 901.1667,-124\"/>\n",
       "<text text-anchor=\"start\" x=\"794.1667\" y=\"-108.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.089</text>\n",
       "<text text-anchor=\"start\" x=\"786.1667\" y=\"-93.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 326</text>\n",
       "<text text-anchor=\"start\" x=\"789.6667\" y=\"-78.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.213</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M704.9342,-97.5C726.1701,-97.5 748.0309,-97.5 767.802,-97.5\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"767.9363,-101.0001 777.9362,-97.5 767.9362,-94.0001 767.9363,-101.0001\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"#f8dcc8\" stroke=\"#000000\" points=\"901.1667,-53 778.1667,-53 778.1667,0 901.1667,0 901.1667,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"794.1667\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.069</text>\n",
       "<text text-anchor=\"start\" x=\"786.1667\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 523</text>\n",
       "<text text-anchor=\"start\" x=\"789.6667\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.432</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M704.9342,-66.3584C726.3649,-60.0184 748.432,-53.4902 768.3457,-47.5991\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"769.3399,-50.955 777.9362,-44.7619 767.3541,-44.2426 769.3399,-50.955\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fce570c04a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_tree(m.estimators_[0], df_trn, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we create a bigger tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.431033801933e-17, 0.4655942451247174, 1.0, 0.6128643362025183]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)\n",
    "\n",
    "#last two are training r^2 and validation r^2 values respectivetly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set result looks great! But the validation set is worse than our original model. This is why we need to use *bagging* of multiple trees to get more generalizable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about bagging in random forests, let's start with our basic model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "?RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09347776585645785, 0.35722257179076583, 0.9810392834933029, 0.772109742245746]\n"
     ]
    }
   ],
   "source": [
    "#n scikit-learn, there is another class called ExtraTreeClassifier which \n",
    "#is an extremely randomized tree model.\n",
    "\n",
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "#See the bag of little bootstraps in notes\n",
    "#by default you can see estimator =10 \n",
    "#which means it will create 10 trees\n",
    "\n",
    "\n",
    "#Extra regressor what differennce is it randomly tries a few \n",
    "#splits of few variables much faster more randomness\n",
    "#co relation between trees should be minimize in order to \n",
    "#generalize\n",
    "\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll grab the predictions for each individual tree, and look at one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each tree is stored in underscore estimoators\n",
    "\n",
    "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n",
    "#above code gives me prediction for each model\n",
    "preds[:,0], np.mean(preds[:,0]), y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape\n",
    "#I have 12000 predictions for each of my 10 tree \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of 10 trees take mean of all of predictions upto ith tree\n",
    "\n",
    "#predict r^2 more bagging the more generalize\n",
    "#Be ware the last point on upside should match \n",
    "#Bagging 1st window\n",
    "#1:07:24 Lesson 2\n",
    "plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this curve suggests that adding more trees isn't going to help us much. Let's check. (Compare this to our original model on a sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doubling and compare r^2 value not help that much\n",
    "#1:08\n",
    "m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=80, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-bag (OOB) score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we've shown, we can't tell. However, random forests have a very clever trick called *out-of-bag (OOB) error* which can handle this (and more!)\n",
    "\n",
    "The idea is to calculate error on the training set, but only include the trees in the calculation of a row's error where that row was *not* included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set.\n",
    "\n",
    "This also has the benefit of allowing us to see whether our model generalizes, even if we only have a small amount of data so want to avoid separating some out to create a validation set.\n",
    "\n",
    "This is as simple as adding one more parameter to our model constructor. We print the OOB error last in our `print_score` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of rows which are not used during random sampling\n",
    "\n",
    "#pass them as validation feature\n",
    "#and so on\n",
    "#different validation set for each tree\n",
    "\n",
    "#oob_score=TRUE  it will create this thing for u\n",
    "#one extra number at the end in print score will do this for you\n",
    "\n",
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)\n",
    "#automated way to set hyperparameters\n",
    "# 1:15 See Speeding up this  I just subset 30000 row and make \n",
    "#all trees with random 30000 rows\n",
    "#Why dont i do all different 30000 each time \n",
    "#rather than bootstraping\n",
    "#all the rows solution is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that our validation set time difference is making an impact, as is model over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that one of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: *subsampling*. Let's return to using our full dataset, so that we can demonstrate the impact of this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\n",
    "X_train, X_valid = split_vals(df_trn, n_trn)\n",
    "y_train, y_valid = split_vals(y_trn, n_trn)\n",
    "\n",
    "len(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a *different* random subset per tree. That way, given enough trees, the model can still see *all* the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not bootstrap means not replacemet =TRUE\n",
    "\n",
    "set_rf_samples(20000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset_rf_samples() see this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you use this approACH oob=false\n",
    "\n",
    "m = RandomForestRegressor(n_jobs=-1, oob_score=True)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each additional tree allows the model to see more data, this approach can make additional trees more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enogh tree will see everything\n",
    "\n",
    "#increase estimator to 40 larger 0.86 to 0.876\n",
    "\n",
    "\n",
    "\n",
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)\n",
    "\n",
    "\n",
    "#Advantage lets say you have 120000 million rows \n",
    "#random forest will be forever you can use this solution instead\n",
    "#So no dataset is too big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree building parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_rf_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a baseline for this full set to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dectree_max_depth(tree):\n",
    "    children_left = tree.children_left\n",
    "    children_right = tree.children_right\n",
    "\n",
    "    def walk(node_id):\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            left_max = 1 + walk(children_left[node_id])\n",
    "            right_max = 1 + walk(children_right[node_id])\n",
    "            return max(left_max, right_max)\n",
    "        else: # leaf\n",
    "            return 1\n",
    "\n",
    "    root_node_id = 0\n",
    "    return walk(root_node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation set score 2nd last answer ob is kast one\n",
    "\n",
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=m.estimators_[0].tree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree_max_depth(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#min sample means stop training the tree further when\n",
    "# when your leaf node has 5 or less sample in possible \n",
    "#1,3,5,10,25\n",
    "\n",
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=m.estimators_[0].tree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree_max_depth(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with `min_samples_leaf`) that we require some minimum number of rows in every leaf node. This has two benefits:\n",
    "\n",
    "- There are less decision rules for each leaf node; simpler models should generalize better\n",
    "- The predictions are made by averaging more rows in the leaf node, resulting in less volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also increase the amount of variation amongst the trees by not only use a sample of rows for each tree, but to also using a sample of *columns* for each *split*. We do this by specifying `max_features`, which is the proportion of features to randomly select from at each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- None\n",
    "- 0.5\n",
    "- 'sqrt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1, 3, 5, 10, 25, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every split we choose different subset of column rather than\n",
    "#looking every possible column 0.5 randomly choose half of then\n",
    "# pass sqrt lg  goog value is 0.5 log2 or sqrt ...see second \n",
    "# row root mean square error of log price drop from 0.234 to 0.2286\n",
    "\n",
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't compare our results directly with the Kaggle competition, since it used a different validation set (and we can no longer to submit to this competition) - but we can at least see that we're getting similar results to the winners based on the dataset we have.\n",
    "\n",
    "The sklearn docs [show an example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html) of different `max_features` methods with increasing numbers of trees - as you see, using a subset of features on each split requires using more trees, but results in better models:\n",
    "![sklearn max_features chart](http://scikit-learn.org/stable/_images/sphx_glr_plot_ensemble_oob_001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.fiProductClassDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.fiProductClassDesc.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.fiProductClassDesc.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We don't implement proximity matrix in Scikit-Learn (yet).\n",
    "\n",
    "#However, this could be done by relying on the apply function provided in our implementation of decision trees. That is, for all pairs of samples in your dataset, iterate over the decision trees in the forest (through forest.estimators_) and count the number of times they fall in the same leaf, i.e., the number of times apply give the same node id for both samples in the pair.\n",
    "\n",
    "#Hope this helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
